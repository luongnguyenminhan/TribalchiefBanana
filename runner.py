from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import os
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

MODEL_NAME = "tarudesu/ViHateT5-base-HSD"

# Global variables Ä‘á»ƒ cache model vÃ  tokenizer
_tokenizer = None
_model = None
_model_loaded = False


def get_model_and_tokenizer():
    """
    Lazy loading cho model vÃ  tokenizer
    Chá»‰ load má»™t láº§n vÃ  cache láº¡i
    """
    global _tokenizer, _model, _model_loaded

    if _model_loaded:
        return _tokenizer, _model

    try:
        logger.info(f"ğŸ”„ Loading model: {MODEL_NAME}")

        # Kiá»ƒm tra cache directory
        cache_dir = os.environ.get("TRANSFORMERS_CACHE", "~/.cache/huggingface")
        logger.info(f"ğŸ“¦ Using cache directory: {cache_dir}")

        # Load tokenizer
        logger.info("ğŸ“ Loading tokenizer...")
        _tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

        # Load model
        logger.info("ğŸ¤– Loading model...")
        _model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)

        _model_loaded = True
        logger.info("âœ… Model and tokenizer loaded successfully!")

        return _tokenizer, _model

    except Exception as e:
        logger.error(f"âŒ Error loading model: {e}")
        raise RuntimeError(f"Failed to load model {MODEL_NAME}: {e}")


def clean_text(text):
    """LÃ m sáº¡ch vÃ  chuáº©n hÃ³a text input"""
    if not isinstance(text, str):
        text = str(text)
    return text.encode("utf-8", "ignore").decode("utf-8").strip()


def check_toxic(text):
    """
    Kiá»ƒm tra Ä‘á»™c tÃ­nh cá»§a vÄƒn báº£n

    Args:
        text (str): VÄƒn báº£n cáº§n kiá»ƒm tra

    Returns:
        str: Káº¿t quáº£ phÃ¢n loáº¡i (CLEAN hoáº·c TOXIC)

    Raises:
        RuntimeError: Náº¿u khÃ´ng thá»ƒ load model
        ValueError: Náº¿u input text khÃ´ng há»£p lá»‡
    """
    if not text or not text.strip():
        raise ValueError("Input text cannot be empty")

    try:
        # Get cached model vÃ  tokenizer
        tokenizer, model = get_model_and_tokenizer()

        # Clean text
        text = clean_text(text)
        prefix = "toxic-speech-detection"
        input_text = f"{prefix}: {text}"

        logger.debug(f"ğŸ” Processing text: {input_text[:50]}...")

        # Tokenize
        input_ids = tokenizer.encode(input_text, return_tensors="pt")

        # Generate prediction
        output_ids = model.generate(input_ids, max_length=256)

        # Decode result
        output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

        logger.debug(f"âœ… Result: {output_text}")
        return output_text

    except Exception as e:
        logger.error(f"âŒ Error in toxicity check: {e}")
        raise


def health_check():
    """
    Kiá»ƒm tra tráº¡ng thÃ¡i cá»§a model

    Returns:
        dict: ThÃ´ng tin tráº¡ng thÃ¡i model
    """
    try:
        tokenizer, model = get_model_and_tokenizer()

        # Test vá»›i input Ä‘Æ¡n giáº£n
        test_result = check_toxic("xin chÃ o")

        return {
            "model_loaded": True,
            "model_name": MODEL_NAME,
            "test_result": test_result,
            "status": "healthy",
        }
    except Exception as e:
        return {
            "model_loaded": False,
            "model_name": MODEL_NAME,
            "error": str(e),
            "status": "unhealthy",
        }


# Legacy support - giá»¯ cho compatibility vá»›i code cÅ©
# Chá»‰ load khi Ä‘Æ°á»£c gá»i trá»±c tiáº¿p
if __name__ == "__main__":
    print("=== TOXICITY CHECK ===")

    # Load model khi cháº¡y trá»±c tiáº¿p
    try:
        tokenizer, model = get_model_and_tokenizer()
        print("ğŸ‰ Model loaded successfully!")
    except Exception as e:
        print(f"âŒ Failed to load model: {e}")
        exit(1)

    # Interactive loop
    while True:
        text = input("Nháº­p vÄƒn báº£n (hoáº·c gÃµ 'q' Ä‘á»ƒ thoÃ¡t): ").strip()
        if text.lower() == "q":
            break
        if not text:
            print("=> Báº¡n chÆ°a nháº­p gÃ¬ cáº£.")
            continue
        try:
            result = check_toxic(text)
            print(f"=> Káº¿t quáº£: {result}")
        except Exception as e:
            print(f"=> ÄÃ£ xáº£y ra lá»—i: {e}")
