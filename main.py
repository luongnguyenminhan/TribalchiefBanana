from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import Optional
import uvicorn
import logging
from runner import check_toxic, health_check

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="ViHateT5 Toxicity Detection API",
    description="API ƒë·ªÉ ki·ªÉm tra ƒë·ªôc t√≠nh trong vƒÉn b·∫£n ti·∫øng Vi·ªát s·ª≠ d·ª•ng model ViHateT5-base-HSD",
    version="2.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

class TextRequest(BaseModel):
    text: str
    
    class Config:
        schema_extra = {
            "example": {
                "text": "Xin ch√†o, b·∫°n kh·ªèe kh√¥ng?"
            }
        }

class APIResponse(BaseModel):
    status_code: int
    error: Optional[str] = None
    data: Optional[dict] = None
    
    class Config:
        schema_extra = {
            "example": {
                "status_code": 200,
                "error": None,
                "data": {
                    "input_text": "Xin ch√†o, b·∫°n kh·ªèe kh√¥ng?",
                    "toxicity_result": "CLEAN",
                    "processed": True
                }
            }
        }

@app.get("/", response_model=APIResponse, tags=["Root"])
async def root():
    """Endpoint g·ªëc - Ki·ªÉm tra API ƒëang ho·∫°t ƒë·ªông"""
    try:
        return APIResponse(
            status_code=200,
            error=None,
            data={
                "message": "ViHateT5 Toxicity Detection API is running",
                "version": "2.0.0",
                "docs": "/docs",
                "health_check": "/health"
            }
        )
    except Exception as e:
        logger.error(f"Error in root endpoint: {e}")
        return APIResponse(
            status_code=500,
            error=f"Internal server error: {str(e)}",
            data=None
        )

@app.post("/check-toxicity", response_model=APIResponse, tags=["Toxicity Detection"])
async def check_toxicity_endpoint(request: TextRequest):
    """
    Ki·ªÉm tra ƒë·ªôc t√≠nh c·ªßa vƒÉn b·∫£n ti·∫øng Vi·ªát
    
    - **text**: VƒÉn b·∫£n c·∫ßn ki·ªÉm tra ƒë·ªôc t√≠nh (b·∫Øt bu·ªôc)
    
    **V√≠ d·ª• request:**
    ```json
    {
        "text": "Xin ch√†o, b·∫°n kh·ªèe kh√¥ng?"
    }
    ```
    
    **Response s·∫Ω tr·∫£ v·ªÅ:**
    - CLEAN: VƒÉn b·∫£n kh√¥ng ƒë·ªôc h·∫°i
    - TOXIC: VƒÉn b·∫£n c√≥ n·ªôi dung ƒë·ªôc h·∫°i
    """
    try:
        # Validate input
        if not request.text or not request.text.strip():
            return APIResponse(
                status_code=400,
                error="Text cannot be empty or whitespace only",
                data=None
            )
        
        # Check length limit
        if len(request.text) > 1000:
            return APIResponse(
                status_code=400,
                error="Text too long. Maximum 1000 characters allowed",
                data=None
            )
        
        logger.info(f"Processing toxicity check for text: {request.text[:50]}...")
        
        # Perform toxicity check
        result = check_toxic(request.text)
        
        logger.info(f"Toxicity check result: {result}")
        
        return APIResponse(
            status_code=200,
            error=None,
            data={
                "input_text": request.text,
                "toxicity_result": result,
                "processed": True,
                "model": "ViHateT5-base-HSD"
            }
        )
    
    except ValueError as ve:
        logger.warning(f"Validation error: {ve}")
        return APIResponse(
            status_code=400,
            error=str(ve),
            data=None
        )
    
    except RuntimeError as re:
        logger.error(f"Model error: {re}")
        return APIResponse(
            status_code=503,
            error=f"Model service unavailable: {str(re)}",
            data=None
        )
    
    except Exception as e:
        logger.error(f"Unexpected error in toxicity check: {e}")
        return APIResponse(
            status_code=500,
            error=f"Internal server error: {str(e)}",
            data=None
        )

@app.get("/health", response_model=APIResponse, tags=["Health Check"])
async def health_check_endpoint():
    """
    Ki·ªÉm tra tr·∫°ng th√°i s·ª©c kh·ªèe c·ªßa server v√† model
    
    Endpoint n√†y ki·ªÉm tra:
    - Tr·∫°ng th√°i server
    - Model ƒë√£ ƒë∆∞·ª£c load hay ch∆∞a
    - Test model v·ªõi input ƒë∆°n gi·∫£n
    """
    try:
        logger.info("Performing health check...")
        
        # Check model health
        model_status = health_check()
        
        if model_status["status"] == "healthy":
            return APIResponse(
                status_code=200,
                error=None,
                data={
                    "server_status": "healthy",
                    "model_status": model_status,
                    "api_version": "2.0.0",
                    "endpoints": {
                        "toxicity_check": "/check-toxicity",
                        "health": "/health",
                        "docs": "/docs"
                    }
                }
            )
        else:
            return APIResponse(
                status_code=503,
                error="Model service unavailable",
                data={
                    "server_status": "healthy",
                    "model_status": model_status,
                    "api_version": "2.0.0"
                }
            )
            
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        return APIResponse(
            status_code=500,
            error=f"Health check failed: {str(e)}",
            data=None
        )

@app.get("/model-info", response_model=APIResponse, tags=["Model Info"])
async def model_info():
    """Th√¥ng tin v·ªÅ model ƒëang s·ª≠ d·ª•ng"""
    try:
        return APIResponse(
            status_code=200,
            error=None,
            data={
                "model_name": "tarudesu/ViHateT5-base-HSD",
                "model_type": "Text-to-Text Generation",
                "task": "Toxic Speech Detection",
                "language": "Vietnamese",
                "base_model": "T5",
                "description": "Fine-tuned T5 model for Vietnamese hate speech detection"
            }
        )
    except Exception as e:
        logger.error(f"Error getting model info: {e}")
        return APIResponse(
            status_code=500,
            error=f"Internal server error: {str(e)}",
            data=None
        )

if __name__ == "__main__":
    logger.info("üöÄ Starting ViHateT5 Toxicity Detection API...")
    uvicorn.run(
        "main:app", 
        host="0.0.0.0", 
        port=8000, 
        reload=True,
        log_level="info"
    ) 