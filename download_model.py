#!/usr/bin/env python3
"""
Script Ä‘á»ƒ download model HuggingFace trong quÃ¡ trÃ¬nh build Docker
Sáº½ download model vÃ o cache Ä‘á»ƒ runtime khÃ´ng cáº§n download
"""

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import os
import sys
import logging

# Setup logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

MODEL_NAME = "tarudesu/ViHateT5-base-HSD"


def setup_cache_directory():
    """Setup vÃ  kiá»ƒm tra cache directory"""
    cache_dir = os.environ.get("TRANSFORMERS_CACHE", "/root/.cache/huggingface")

    # Táº¡o cache directory náº¿u chÆ°a cÃ³
    os.makedirs(cache_dir, exist_ok=True)
    logger.info(f"ğŸ“ Cache directory: {cache_dir}")

    return cache_dir


def download_model():
    """Download model vÃ  tokenizer vÃ o cache"""
    logger.info(f"ğŸ”„ Starting download for model: {MODEL_NAME}")

    try:
        # Setup cache directory
        cache_dir = setup_cache_directory()

        # Download tokenizer
        logger.info("ğŸ“ Downloading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, cache_dir=cache_dir)
        logger.info("âœ… Tokenizer downloaded successfully")

        # Download model
        logger.info("ğŸ¤– Downloading model...")
        model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, cache_dir=cache_dir)
        logger.info("âœ… Model downloaded successfully")

        # Test model Ä‘á»ƒ Ä‘áº£m báº£o hoáº¡t Ä‘á»™ng
        logger.info("ğŸ§ª Testing model functionality...")
        test_cases = ["xin chÃ o", "báº¡n khá»e khÃ´ng", "cáº£m Æ¡n báº¡n"]

        for test_text in test_cases:
            test_input = f"toxic-speech-detection: {test_text}"
            input_ids = tokenizer.encode(test_input, return_tensors="pt")

            # Generate vá»›i cÃ¡c tham sá»‘ tá»‘i Æ°u
            output_ids = model.generate(
                input_ids, max_length=256, do_sample=False, early_stopping=True
            )

            output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)
            logger.info(f"âœ… Test '{test_text}' -> '{output_text}'")

        # Kiá»ƒm tra cache size
        try:
            cache_size = get_directory_size(cache_dir)
            logger.info(f"ğŸ“¦ Cache size: {cache_size:.2f} MB")
        except Exception as e:
            logger.warning(f"Could not calculate cache size: {e}")

        # Environment info
        logger.info(f"ğŸ“¦ Model cached in: {cache_dir}")
        logger.info(f"ğŸ Python version: {sys.version}")

        return True

    except Exception as e:
        logger.error(f"âŒ Error downloading model: {e}")
        logger.error(f"Error type: {type(e).__name__}")
        return False


def get_directory_size(path):
    """TÃ­nh kÃ­ch thÆ°á»›c directory tÃ­nh báº±ng MB"""
    total_size = 0
    try:
        for dirpath, dirnames, filenames in os.walk(path):
            for filename in filenames:
                filepath = os.path.join(dirpath, filename)
                if os.path.exists(filepath):
                    total_size += os.path.getsize(filepath)
    except Exception as e:
        logger.warning(f"Error calculating directory size: {e}")
        return 0

    return total_size / (1024 * 1024)  # Convert to MB


def verify_model_files():
    """Verify ráº±ng model files Ä‘Ã£ Ä‘Æ°á»£c download thÃ nh cÃ´ng"""
    cache_dir = os.environ.get("TRANSFORMERS_CACHE", "/root/.cache/huggingface")

    logger.info("ğŸ” Verifying downloaded model files...")

    # CÃ¡c files quan trá»ng cáº§n cÃ³
    expected_patterns = [
        "config.json",
        "tokenizer.json",
        "pytorch_model.bin",
        "special_tokens_map.json",
    ]

    found_files = []
    for root, dirs, files in os.walk(cache_dir):
        found_files.extend(files)

    # Check if important files exist
    for pattern in expected_patterns:
        matching_files = [f for f in found_files if pattern in f]
        if matching_files:
            logger.info(f"âœ… Found {pattern}: {len(matching_files)} files")
        else:
            logger.warning(f"âš ï¸  Missing {pattern}")

    logger.info(f"ğŸ“Š Total files in cache: {len(found_files)}")
    return len(found_files) > 0


def main():
    """Main function"""
    logger.info("ğŸš€ Starting ViHateT5 model download process...")

    try:
        # Download model
        success = download_model()

        if not success:
            logger.error("âŒ Model download failed!")
            sys.exit(1)

        # Verify files
        if verify_model_files():
            logger.info("âœ… Model files verification passed!")
        else:
            logger.warning("âš ï¸  Model files verification had issues")

        logger.info("ğŸ‰ Model download completed successfully!")
        logger.info("ğŸ“ Model is ready for use in production!")

    except KeyboardInterrupt:
        logger.info("ğŸ›‘ Download interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"ğŸ’¥ Unexpected error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
